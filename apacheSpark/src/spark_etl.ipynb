{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bca02e1-595c-4799-92b7-59bca8465943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.2.0 in /opt/conda/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /opt/conda/lib/python3.12/site-packages (from pyspark==3.2.0) (0.10.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.2.0, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bec9399-170a-4c12-8d52-7f8fd7d74d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0b4c96-45e6-40e8-9b57-fd06c8bc886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_jar = \"/usr/local/spark/jars/postgresql-42.3.8.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cfea079-10b6-4942-ae7b-adbf9a2a5011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/01/23 00:05:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/23 00:05:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"An Spark application\")\n",
    "    .config(\"spark.jars\", postgres_jar)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462ab0be-3f72-4ea1-82e4-36e12410d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://postgres-db:5432/downstream\"\n",
    "db_properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"secret\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c7d59c-e0da-4ffc-a3e2-9ad6f308a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Game\", StringType(), nullable=True),\n",
    "    StructField(\"Year\", IntegerType(), nullable=True),\n",
    "    StructField(\"Genre\", StringType(), nullable=True),\n",
    "    StructField(\"Publisher\", StringType(), nullable=True),\n",
    "    StructField(\"North America\", DoubleType(), nullable=True),\n",
    "    StructField(\"Europe\", DoubleType(), nullable=True),\n",
    "    StructField(\"Japan\", DoubleType(), nullable=True),\n",
    "    StructField(\"Rest of world\", DoubleType(), nullable=True),\n",
    "    StructField(\"Global\", DoubleType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31585550-0cdd-4fbc-853f-7b9d1e96d37c",
   "metadata": {},
   "source": [
    "## Creando la tabla de videojuegos en PSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3edefed7-319f-4399-9728-d85b618bf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"/home/jovyan/data/PS4_GamesSales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99403ad9-ecfb-407b-933b-07eb3789ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.jdbc(url=jdbc_url, table=\"Games\", mode=\"overwrite\", properties=db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b48d9a-789c-4f1b-ba50-b7e87f6659ce",
   "metadata": {},
   "source": [
    "## ETL con Spark\n",
    "Hagamos una ETL usando Spark.\n",
    "1. Extraemos de MongoDB, nuestro upstream.\n",
    "2. Realicemos las siguientes transformaciones sobre los datos:\n",
    "3. Cargamos a PostgreSQL, nuestro downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "216d51d0-1f51-44e5-a715-192e1df38ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pyspark.sql import DataFrame, Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b6d1e4d-18d3-42e6-a933-b96cfa0163b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RequestConf:\n",
    "    __date_from: str\n",
    "    __date_to: str\n",
    "    __search_term: str\n",
    "    __api_key: str = \"4f1e6cf579d44b7db3fe6d6505c54b2b\"\n",
    "    url: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.url = f\"https://newsapi.org/v2/everything?q={self.__search_term}&from={self.__date_from}&to={self.__date_to}&apiKey={self.__api_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a117569-f94d-4a28-8455-1480a1eb46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoadConf:\n",
    "    url: str\n",
    "    table: str\n",
    "    mode: str\n",
    "    properties: dict\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.mode.lower() in [\"overwrite\", \"append\"], f\"Mode {self.mode} not supported yet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3779b11e-b0c7-4325-83ce-b6f5d53abb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(req_conf: RequestConf) -> dict:\n",
    "    res:dict =  get(req_conf.url).json()\n",
    "    try:\n",
    "        return res[\"articles\"]\n",
    "    except KeyError:\n",
    "        raise Exception(\"Not found key 'articles' in response from news API.\")\n",
    "\n",
    "def register_udf(req_conf: RequestConf):\n",
    "    res_schema = ArrayType(StructType([\n",
    "        StructField(\"title\", StringType()),\n",
    "        StructField(\"description\", StringType()),\n",
    "        StructField(\"url\", StringType()),\n",
    "        StructField(\"publishedAt\", TimestampType()),\n",
    "        StructField(\"content\", StringType())\n",
    "    ]))\n",
    "    return udf(lambda: get_news(req_conf), res_schema)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76fbe253-1e71-41e7-a667-09909406d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    class Extract:\n",
    "        def __init__(self, spark: SparkSession, req_conf: RequestConf):\n",
    "            assert isinstance(req_conf, RequestConf), \"Passed configurations must be instance of RequestConf\"\n",
    "            self.spark = spark\n",
    "            self.req_conf = req_conf\n",
    "\n",
    "        \n",
    "        def run(self) -> DataFrame:\n",
    "            #udf_get_news = register_udf(self.req_conf)\n",
    "            row = Row(\"empty\")\n",
    "            input_df = self.spark.createDataFrame([row(\"GET\")])\n",
    "            \n",
    "    \n",
    "    class Load:\n",
    "        def __init__(self, load_conf: LoadConf):\n",
    "            assert isinstance(load_conf, LoadConf), \"Passed configurations must be instance of LoadConf\"\n",
    "            self.load_conf = load_conf\n",
    "        \n",
    "        def run(self, df: DataFrame) -> None:\n",
    "            assert isinstance(df, DataFrame)\n",
    "            df.write.jdbc(\n",
    "                url=self.load_conf.url, \n",
    "                table=self.load_conf.table, \n",
    "                mode=self.load_conf.mode, \n",
    "                properties=self.load_conf.properties\n",
    "            )\n",
    "            \n",
    "    def __init__(self, req_conf: RequestConf, load_conf: LoadConf):\n",
    "        spark = (\n",
    "            SparkSession\n",
    "            .builder\n",
    "            .appName(\"An Spark application\")\n",
    "            .config(\"spark.jars\", postgres_jar)\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        self.extract = Pipeline.Extract(spark, req_conf)\n",
    "        self.load = Pipeline.Load(load_conf)\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        df: DataFrame = self.extract.run()\n",
    "        return df\n",
    "        #self.load.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2df4c97d-be36-4ed2-b18c-214f81f49ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(*args):\n",
    "    db_properties = {\n",
    "        \"user\": \"admin\",\n",
    "        \"password\": \"secret\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    for search_term in args:\n",
    "        req_conf = RequestConf(\"2025-01-19\", \"2025-01-20\", search_term)\n",
    "        load_conf = LoadConf(\"jdbc:postgresql://postgres-db:5432/downstream\", \"news\", \"overwrite\", db_properties)\n",
    "        pipeline = Pipeline(req_conf, load_conf)\n",
    "        pipeline.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1eaa85f6-842b-4636-8ad3-0cde30ee2f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 653, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 526, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:563\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:653\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# distpatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:526\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:507\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    506\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 507\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/cloudpickle/cloudpickle.py:236\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m names \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_names\n\u001b[0;32m--> 236\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresonding to the one\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrump\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[106], line 11\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      9\u001b[0m load_conf \u001b[38;5;241m=\u001b[39m LoadConf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://postgres-db:5432/downstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, db_properties)\n\u001b[1;32m     10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(req_conf, load_conf)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[112], line 41\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     df: DataFrame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[112], line 12\u001b[0m, in \u001b[0;36mPipeline.Extract.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#udf_get_news = register_udf(self.req_conf)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     row \u001b[38;5;241m=\u001b[39m Row(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     input_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/session.py:701\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[0;32m--> 701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    703\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:2620\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Return a JavaRDD of Object by unpickling\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \n\u001b[1;32m   2616\u001b[0m \u001b[38;5;124;03mIt will convert each Python object into Java object by Pyrolite, whenever the\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;124;03mRDD is serialized in batch or not.\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2619\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[0;32m-> 2620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:2951\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2949\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2951\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2953\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func,\n\u001b[1;32m   2954\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier)\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_val \u001b[38;5;241m=\u001b[39m python_rdd\u001b[38;5;241m.\u001b[39masJavaRDD()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:2830\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2829\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 2830\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[1;32m   2832\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/rdd.py:2816\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 2816\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "main(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7558dd2d-6c32-4af8-bb1b-d1d028f3a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(url: str, search_term: str, date_from: str, api_key: str):\n",
    "    return get(url.format(search_term, date_from, api_key)).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb8ed03d-6217-4936-8bb9-e3a38d4ac283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': {'id': None, 'name': 'Newsbreak.com'},\n",
       "  'author': 'Brett Rowland | The Center Square',\n",
       "  'title': \"Report: Federal government can't fully account for its 'unsustainable' spending - NewsBreak\",\n",
       "  'description': \"(The Center Square) – A Congressional watchdog says it is again unable to determine if the federal government's financial statements are reliable. The fed\",\n",
       "  'url': 'https://www.newsbreak.com/share/3767247761179-report-federal-government-can-t-fully-account-for-its-unsustainable-spending?_f=app_share&pd=0GOkyYtn&lang=en_US&send_time=1737245806&trans_data=%7B%22platform%22%3A0%2C%22cv%22%3A%2225.3.0.29%22%2C%22languages%22%3A%22en%22%7D&sep=ns_foryou_blend_exp_25q1-v2%2Cns_foryou_model_exp_25q1-v2%2Cns_foryou_rank_exp_25q1-v3%2Cns_foryou_recall_exp_25q1-v5%2Cns_push_exp_rt_bucketv12-v1&s=i2',\n",
       "  'urlToImage': 'https://img.particlenews.com/img/id/3CHFvx_0yFyNcEn00',\n",
       "  'publishedAt': '2025-01-19T00:24:36Z',\n",
       "  'content': \"(The Center Square) A Congressional watchdog says it is again unable to determine if the federal government's financial statements are reliable. \\r\\n The federal government reported net costs of $7.4 t… [+2282 chars]\"},\n",
       " {'source': {'id': None, 'name': 'Biztoc.com'},\n",
       "  'author': 'youtube.com',\n",
       "  'title': 'Billionaire bromance: Elon Musk & Jeff Bezos team up',\n",
       "  'description': 'Is there a budding bromance between Elon Musk & Jeff Bezos? After years of rivalry, the two billionaires are exchanging ...',\n",
       "  'url': 'https://biztoc.com/x/feed5ff71e23873f',\n",
       "  'urlToImage': 'https://biztoc.com/cdn/feed5ff71e23873f_s.webp',\n",
       "  'publishedAt': '2025-01-19T00:17:44Z',\n",
       "  'content': 'Is there a budding bromance between Elon Musk &amp; Jeff Bezos? After years of rivalry, the two billionaires are exchanging ...\\r\\nThis story appeared on youtube.com, 2025-01-18 23:12:37.'},\n",
       " {'source': {'id': None, 'name': 'Newsbreak.com'},\n",
       "  'author': 'Brett Rowland | The Center Square',\n",
       "  'title': \"Report: Federal government can't fully account for its 'unsustainable' spending - NewsBreak\",\n",
       "  'description': \"(The Center Square) – A Congressional watchdog says it is again unable to determine if the federal government's financial statements are reliable. The fed\",\n",
       "  'url': 'https://www.newsbreak.com/share/3767247761179-report-federal-government-can-t-fully-account-for-its-unsustainable-spending?_f=app_share&pd=00blTmeh&lang=en_US&send_time=1737245681&trans_data=%7B%22platform%22%3A0%2C%22cv%22%3A%2225.3.0.29%22%2C%22languages%22%3A%22en%22%7D&sep=ns_foryou_model_exp_25q1-v2%2Cns_foryou_recall_exp_25q1-v5%2Cns_push_exp_rt_bucketv12-v5%2Cns_foryou_blend_exp_25q1-v4%2Cns_foryou_rank_exp_25q1-v9&s=i2',\n",
       "  'urlToImage': 'https://img.particlenews.com/img/id/3CHFvx_0yFyNcEn00',\n",
       "  'publishedAt': '2025-01-19T00:16:09Z',\n",
       "  'content': \"(The Center Square) A Congressional watchdog says it is again unable to determine if the federal government's financial statements are reliable. \\r\\n The federal government reported net costs of $7.4 t… [+2282 chars]\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_url = \"https://newsapi.org/v2/everything?q={}&from={}&sortBy=publishedAt&apiKey={}\"\n",
    "get_news(news_url, \"elon musk\", \"2024-12-20\", API_KEY)[\"articles\"][0:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
